# -*- coding: utf-8 -*-
"""HealthInsurance 2.10update.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dn1IO1CMFpsS1WUI6GZ_c4D1YAkMD5K0

Library Import
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

import seaborn as sns
import matplotlib.pyplot as plt

import math
import pandas as pd
import numpy as np
from tqdm import tqdm

from sklearn import metrics
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, f1_score
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA
from collections import Counter
from sklearn.manifold import TSNE
from sklearn.utils.extmath import randomized_svd
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

from prettytable import PrettyTable
from sklearn.model_selection import StratifiedKFold
from imblearn.over_sampling import SMOTE

"""Data Import"""

# Load Dataset

df_label = pd.read_csv("Train.csv")
Inpatientdata=pd.read_csv("Train_Inpatientdata.csv")
Outpatientdata=pd.read_csv("Train_Outpatientdata.csv")
Beneficiarydata=pd.read_csv("Train_Beneficiarydata.csv")

"""Shape of Data"""

# Shape of data

print("The Training inpatient data has {} records and {} fields. \n" .format(Inpatientdata.shape[0], Inpatientdata.shape[1]))
print("The Training outpatient data has {} records and {} fields. \n" .format(Outpatientdata.shape[0], Outpatientdata.shape[1]))
print("The Training Benficiary data has {} records and {} fields. \n" .format(Beneficiarydata.shape[0], Beneficiarydata.shape[1]))

# Code Reference: https://www.geeksforgeeks.org/numpy-ones-python/

all_ones = np.ones(len(Inpatientdata), dtype = int)                             # Add all ones for all the hospitalised/inpatient beneficiaries
Inpatientdata['IsHospitalized'] = list(all_ones)                                # Add new feature whether a beneficiary is inpatient or outpatient type

all_zeros = np.zeros(len(Outpatientdata), dtype = int)                           # Add all zeros for all the outpatient beneficiaries
Outpatientdata['IsHospitalized'] = list(all_zeros)                               # Add new feature whether a beneficiary is inpatient or outpatient type

# Code Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html

common_lst = []

for col in Outpatientdata.columns:                                               # Find common columns in Inpatient and Outpatient Data
  if col in Inpatientdata.columns:
    common_lst.append(col)                                                  # Add the common columns in new list

print(common_lst)

# Merge the Inpatient and Outpatient Data on the common columns list
in_out_patient = pd.merge(Inpatientdata, Outpatientdata, left_on = common_lst, right_on = common_lst, how = 'outer')
in_out_patient.head()

# Merge the clubbed data(Inpatient/Outpatient) with Beneficiary Data on the Beneficiary ID

ben_inout_patient = pd.merge(in_out_patient, Beneficiarydata, left_on = 'BeneID', right_on = 'BeneID', how = 'inner')
ben_inout_patient.head()

# Code Reference: https://www.geeksforgeeks.org/python-pandas-merging-joining-and-concatenating/
# Merge the clubbed data(Inpatient/Outpatient and Beneficiary) with Train Class Labels on the Provider ID

train_final = pd.merge(ben_inout_patient, df_label , how = 'inner', on = 'Provider' )
train_final.head()

print('Shape of Train Final Data:', train_final.shape)                  # Shape of the Train Final Data
print('-'*50)
print('Unique values in Train Final Data Column:')                      # Check for unique value counts for Train Final Data Columns
print(train_final.nunique())

train_final = train_final.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,
                           'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2,
                           'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2,
                           'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2, 'Gender': 2 }, 0)
train_final = train_final.replace({'RenalDiseaseIndicator': 'Y'}, 1)
train_final['RenalDiseaseIndicator'] = train_final['RenalDiseaseIndicator'].apply(pd.to_numeric)          # convert object datatype to numeric datatype
train_final.head()

train_final['claim_strt_year'] = pd.DatetimeIndex(train_final['ClaimStartDt']).year    # Add new feature claim start year of the admitted beneficiary
train_final['adm_year'] = pd.DatetimeIndex(train_final['AdmissionDt']).year            # Add new feature admit year of the beneficiary
# Add new feature number of days of applied claim of the admitted beneficiary
train_final['Claim_Days'] = (pd.DatetimeIndex(train_final['ClaimEndDt']) - pd.DatetimeIndex(train_final['ClaimStartDt'])).days

# Add new feature number of days spent in hospital by the beneficiary
train_final['Hospital_Days'] = (pd.DatetimeIndex(train_final['DischargeDt']) - pd.DatetimeIndex(train_final['AdmissionDt'])).days
train_final['Difference'] = np.where(train_final['Claim_Days']>train_final['Hospital_Days'], train_final['Claim_Days'] - train_final['Hospital_Days'], 0)
train_final['birth_year'] = pd.DatetimeIndex(train_final['DOB']).year              # Add new feature birth year of the beneficiary
train_final['IsAlive'] = pd.factorize(train_final['DOD'])[0]                # Add new feature if a beneficiary is Alive or not(DOD feature)?
train_final['IsAlive'] = np.where(train_final['IsAlive'] == -1, 1, 0)

out = pd.DatetimeIndex(train_final.DOD).max()                                   # Calculate the max date from DOD column

train_final['Age'] = ((out - pd.DatetimeIndex(train_final['DOB'])).days)/365    # Calculate age from max DOD and DOB column
train_final['Age'] = train_final['Age'].round().astype(int)                     # convert age into int

lst_age = list(train_final['Age'].values)                       # Take age as a list to divide it into age-category
lst_category = []
for i in lst_age:
  if i<=45:
    lst_category.append('Young')                                # If age <= 45, then age-category = 'young'
  elif i<=64:
    lst_category.append('Adult')                                # If age <= 64, then age-category = 'adult'
  elif i<=83:
    lst_category.append('Old')                                  # If age <= 83, then age-category = 'old'
  else:
    lst_category.append('Very Old')                             # If age > 84, then age-category = 'very old'

train_final['Age_Category'] = lst_category                      # Add the age-category in the dataframe

# Create a new dataframe with all the chronic diseases

new_df = train_final[['RenalDiseaseIndicator', 'ChronicCond_KidneyDisease', 'ChronicCond_Heartfailure', 'ChronicCond_IschemicHeart', 'ChronicCond_Alzheimer', 'ChronicCond_Cancer', 'ChronicCond_ObstrPulmonary','ChronicCond_Depression','ChronicCond_Diabetes','ChronicCond_Osteoporasis','ChronicCond_rheumatoidarthritis','ChronicCond_stroke']].copy()
train_final['Risk'] = new_df.sum(axis = 1)                       # Find how many beneficiaries are suffering from how many chronic disease?
train_final.head()

print('Shape of the final train dataset:', train_final.shape, '\n')             # Shape of the Train Final Data after adding new features
print('Column names in the final train dataset:', train_final.columns)
print('The columns containing NULL values are as follows:')
train_final.columns[train_final.isnull().any()].tolist()

# Code Reference: https://www.geeksforgeeks.org/python-pandas-dataframe-fillna-to-replace-null-values-in-dataframe/

train_final.fillna(value=0, inplace=True)                                       # fill NULL values with 0 for all the columns obtained above

# check if any column is still remaining with NULL values or not?
print('Columns that contain NULL values even after filling it with 0: ', train_final.columns[train_final.isnull().any()].tolist())

def feature_interaction(train_data, group_col, opr_col, task):
    '''Function takes train data groups it using group_col feature and performs task(eg: mean, count) on opr_col'''
    for val in opr_col:
        new_feature = task+'_'+val+'_'+'per'+''.join(group_col)                         # column name of new feature
        train_data[new_feature] = train_data.groupby(group_col)[val].transform(task)    # group by columns to get the new feature
    return train_data

columns = ['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'IPAnnualReimbursementAmt', 'IPAnnualDeductibleAmt', 'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt', 'Age', 'Hospital_Days', 'Claim_Days', 'Risk']

train_final = feature_interaction(train_final, ['Provider'], columns, 'mean')                   # function call
train_final = feature_interaction(train_final, ['BeneID'], columns, 'mean')                     # function call
train_final = feature_interaction(train_final, ['AttendingPhysician'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['OperatingPhysician'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['OtherPhysician'], columns, 'mean')             # function call
train_final = feature_interaction(train_final, ['DiagnosisGroupCode'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmAdmitDiagnosisCode'], columns, 'mean')      # function call
train_final = feature_interaction(train_final, ['ClmProcedureCode_1'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmProcedureCode_2'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmProcedureCode_3'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmProcedureCode_4'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmProcedureCode_5'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmProcedureCode_6'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_1'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_2'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_3'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_4'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_5'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_6'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_7'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_8'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_9'], columns, 'mean')         # function call
train_final = feature_interaction(train_final, ['ClmDiagnosisCode_10'], columns, 'mean')        # function call
train_final = feature_interaction(train_final, ['Provider'], ['ClaimID'], 'count')              # function call
train_final.head()

group_col = ['BeneID', 'AttendingPhysician', 'OtherPhysician', 'OperatingPhysician', 'ClmAdmitDiagnosisCode', 'ClmProcedureCode_1',
               'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4', 'ClmProcedureCode_5', 'ClmProcedureCode_5',
               'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6',
               'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'DiagnosisGroupCode']
for col in group_col:
    lst = ['Provider', col]
    train_final =  feature_interaction(train_final, lst, ['ClaimID'], 'count')

train_final.head()

remove_col=['BeneID', 'ClaimID', 'ClaimStartDt','ClaimEndDt','AttendingPhysician','OperatingPhysician', 'OtherPhysician',
                'ClmDiagnosisCode_1','ClmDiagnosisCode_2', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4','ClmDiagnosisCode_5',
                'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7','ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10',
                'ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3','ClmProcedureCode_4', 'ClmProcedureCode_5',
                'ClmProcedureCode_6','ClmAdmitDiagnosisCode', 'AdmissionDt','claim_strt_year', 'adm_year', 'DischargeDt', 'DiagnosisGroupCode',
                'DOB', 'DOD','birth_year','State', 'County']

train_final=train_final.drop(columns=remove_col, axis=1)
df_all1 = pd.get_dummies(train_final, columns=['Gender', 'Race'])
df_all1.head()

"""My part"""

import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score,precision_score,recall_score, classification_report, confusion_matrix
import missingno as msno
from sklearn.preprocessing import OneHotEncoder
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

import warnings
warnings.simplefilter("ignore")

# histogram of Fraud_reported
px.histogram(df_all1, x="PotentialFraud",color='PotentialFraud',title="PotentialFraud", height=500, width=700)

"""# Data Preprocessing"""

# Change the 'PotentialFraud' into 0 and 1
df_all1['PotentialFraud'] = df_all1['PotentialFraud'].map({'Yes': 1, 'No': 0})

X = df_all1.drop(columns = ['Provider', 'PotentialFraud'], axis=1)
y = df_all1['PotentialFraud']

"""2. One-Hot Encoding and standardization"""

# One hot encode categorical variable
# Categorical varible: ChronicCond_ features
# Categorical variable: 'Adimtted', 'RenalDiseaseIndicator','ClmDiagnosisCodeIndex','ClmProcedureCodeIndex'

categorical_cols = [col for col in X.columns if col.startswith('ChronicCond_')]
additional_categorical_cols = ['Age_Category']
categorical_cols.extend(additional_categorical_cols)

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

encoded_data = encoder.fit_transform(X[categorical_cols])

encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())

# X = X.reset_index(drop=True)
X = pd.concat([X.drop(categorical_cols, axis=1), encoded_df], axis=1)

"""# Feature Engineering

3. PCA for dimensionality Reduction
"""

# from sklearn.decomposition import PCA

# # Perform PCA, keeping 95% of variance
# pca = PCA(n_components=0.95)
# X_pca = pca.fit_transform(X)

# print(f"Original features: {X.shape[1]}, Reduced features: {X_pca.shape[1]}")

# X_pca = pd.DataFrame(X_pca, columns=[f"PC{i+1}" for i in range(X_pca.shape[1])])

"""4. Lasso Regularization (L1 Penalty) for feature selection"""

# from sklearn.linear_model import LassoCV
# lasso_cv = LassoCV(cv=5, alphas=np.logspace(-4, 0, 10))  # 选最优 alpha
# lasso_cv.fit(X_pca, y)
# best_alpha = lasso_cv.alpha_
# print(f"Best alpha: {best_alpha}")

# from sklearn.linear_model import Lasso
# from sklearn.preprocessing import StandardScaler

# # Standardize the dataset (Lasso requires scaling)
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X_pca)

# # Train Lasso model
# lasso = Lasso(alpha=0.1)  # Adjust alpha as needed
# lasso.fit(X_scaled, y)

# # Identify selected features
# selected_features = X_pca.columns[lasso.coef_ != 0]

# print(f"Selected {len(selected_features)} features using Lasso.")
# X_lasso = X_pca[selected_features]  # Keep only selected features

"""5. Feature Changes"""

# print(f"Original feature count: {X.shape[1]}")
# print(f"After PCA: {X_pca.shape[1]}")
# print(f"After Lasso selection: {X_lasso.shape[1]}")

X.to_csv('X.csv', index=False)
y.to_csv('y.csv', index=False)

"""# Modeling"""

# Split data into train and validation set

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)

# split to training (40%), validation (20%) and testing (20%)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(X_train.shape, X_val.shape, X_test.shape)

"""1. Decision Tree"""

import os
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc

# Initialize Decision Tree model
dt_model = DecisionTreeClassifier(max_depth=10, max_features=0.8, random_state=42)

# Train the model
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_train = dt_model.predict(X_train)
y_pred_test = dt_model.predict(X_test)

# Get probability estimates for AUC calculation
y_prob_train = dt_model.predict_proba(X_train)[:, 1]
y_prob_test = dt_model.predict_proba(X_test)[:, 1]

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)

# Compute AUC-ROC for train and test sets
auc_roc_train = roc_auc_score(y_train, y_prob_train)
auc_roc_test = roc_auc_score(y_test, y_prob_test)

# Print evaluation results
print(f"Decision Tree Model Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Train AUC-ROC: {auc_roc_train:.4f}")
print(f"Test AUC-ROC: {auc_roc_test:.4f}")

# Plot ROC curve for both Train and Test sets
fpr_train, tpr_train, _ = roc_curve(y_train, y_prob_train)
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test)

plt.figure(figsize=(8,6))
plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'Train AUC = {auc_roc_train:.4f}')
plt.plot(fpr_test, tpr_test, color='red', lw=2, label=f'Test AUC = {auc_roc_test:.4f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Decision Tree')
plt.legend(loc="lower right")

# Save the plot to the local desktop
desktop_path = os.path.join("decision_tree_roc_curve.png")
plt.savefig(desktop_path, dpi=300)  # Save with high resolution
plt.show()

print(f"ROC curve saved to: {desktop_path}")

"""2. Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Initialize Random Forest model with your parameters
rf = RandomForestClassifier(
    n_estimators=1000,  # Number of trees
    max_depth=40,       # Maximum depth of trees
    max_features=0.6,   # Number of features to consider for each split
    random_state=42,
    n_jobs=-1           # Use all processors for faster training
)

# Train the model
rf.fit(X_train, y_train)

# Make predictions
y_pred = rf.predict(X_test)
y_prob = rf.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_prob)

# Print evaluation results
print(f"Random Forest Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# Plot AUC-ROC curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend(loc="lower right")
plt.show()

"""3. XGBoost"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt

# Initialize XGBoost model with your parameters
xgb = XGBClassifier(
    learning_rate=1,   # Learning rate
    n_estimators=10,   # Number of boosting rounds
    use_label_encoder=False,  # Avoid warning messages
    eval_metric="logloss",    # Prevent default warning
    random_state=42
)

# Train the model
xgb.fit(X_train, y_train)

# Make predictions
y_pred = xgb.predict(X_test)
y_prob = xgb.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_prob)

# Print evaluation results
print(f"XGBoost Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# Plot AUC-ROC curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - XGBoost')
plt.legend(loc="lower right")
plt.show()

"""4. Gaussian NB"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize GaussianNB model with your parameter
gnb = GaussianNB(var_smoothing=0.0001)

# Train the model
gnb.fit(X_train, y_train)

# Make predictions
y_pred = gnb.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Gaussian Naïve Bayes Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""5. LR"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize Logistic Regression model with your parameters
log_reg = LogisticRegression(C=100, tol=0.01, random_state=42)

# Train the model
log_reg.fit(X_train, y_train)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Logistic Regression Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

!pip install --extra-index-url=https://pypi.nvidia.com cuml-cu12 --upgrade

from cuml.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import cupy as cp

# Convert data to GPU arrays
X_train_gpu = cp.asarray(X_train)
y_train_gpu = cp.asarray(y_train)
X_test_gpu = cp.asarray(X_test)
y_test_gpu = cp.asarray(y_test)

# Initialize GPU Random Forest
rf = RandomForestClassifier(
    n_estimators=1000,  # Number of trees
    max_depth=40,       # Maximum depth of trees
    max_features=0.6,   # Number of features to consider for each split
    random_state=42
)

# Train the model on GPU
rf.fit(X_train_gpu, y_train_gpu)

# Make predictions on GPU
y_pred_gpu = rf.predict(X_test_gpu)

# Convert predictions back to CPU for evaluation
y_pred = cp.asnumpy(y_pred_gpu)
y_test = cp.asnumpy(y_test_gpu)

# Evaluate the model using sklearn (since cuML lacks these metrics)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"GPU Random Forest Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Apply SMOTE to only the training set
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Display class distribution before and after SMOTE
print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", pd.Series(y_train_resampled).value_counts())

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize Logistic Regression model with your parameters
log_reg = LogisticRegression(C=100, tol=0.01, random_state=42)

# Train the model
log_reg.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Logistic Regression Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Install RAPIDS (Only needed in Google Colab)
!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com

import cudf  # GPU-accelerated DataFrame
from cuml.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert data to GPU-accelerated cudf DataFrames
X_train_resampled = cudf.DataFrame(X_train_resampled)
y_train_resampled = cudf.Series(y_train_resampled)
X_test = cudf.DataFrame(X_test)
y_test = cudf.Series(y_test)

# Initialize GPU-based Logistic Regression model
log_reg = LogisticRegression(C=100, tol=0.01, random_state=42)

# Train the model on GPU
log_reg.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = log_reg.predict(X_test).to_pandas()  # Convert back to pandas for metrics

# Convert y_test to pandas for comparison
y_test = y_test.to_pandas()

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Logistic Regression Results (GPU-accelerated with cuML):")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

import cudf  # GPU DataFrame
import cupy as cp  # GPU array library
from cuml.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert data to GPU-accelerated cudf DataFrames
X_train = cudf.DataFrame(X_train)
y_train = cudf.Series(y_train)
X_test = cudf.DataFrame(X_test)
y_test = cudf.Series(y_test)  # Keep y_test as cuDF Series for now

# Convert data to CuPy arrays (for GPU processing)
X_train_cupy = cp.asarray(X_train.to_numpy())
y_train_cupy = cp.asarray(y_train.to_numpy())
X_test_cupy = cp.asarray(X_test.to_numpy())

# Initialize GPU-based Logistic Regression model
log_reg = LogisticRegression(C=100, tol=0.01, random_state=42)

# Train the model on GPU
log_reg.fit(X_train_cupy, y_train_cupy)

# Make predictions (output will be a CuPy array)
y_pred = log_reg.predict(X_test_cupy)

# Convert predictions and labels to NumPy for sklearn metrics
y_pred = cp.asnumpy(y_pred)
y_test = y_test.to_numpy()

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Logistic Regression Results (GPU-accelerated with cuML):")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize GaussianNB model with your parameter
gnb = GaussianNB(var_smoothing=0.0001)

# Train the model
gnb.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = gnb.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Gaussian Naïve Bayes Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize Random Forest model with your parameters
rf = RandomForestClassifier(
    n_estimators=1000,  # Number of trees
    max_depth=40,       # Maximum depth of trees
    max_features=0.6,   # Number of features to consider for each split
    random_state=42,
    n_jobs=-1           # Use all processors for faster training
)

# Train the model
rf.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation results
print(f"Random Forest Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

import os
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc

# Initialize Decision Tree model
dt_model = DecisionTreeClassifier(max_depth=10, max_features=0.8, random_state=42)

# Train the model using resampled dataset
dt_model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_train = dt_model.predict(X_train_resampled)
y_pred_test = dt_model.predict(X_test)

# Get probability estimates for AUC calculation
y_prob_train = dt_model.predict_proba(X_train_resampled)[:, 1]
y_prob_test = dt_model.predict_proba(X_test)[:, 1]

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)

# Compute AUC-ROC for train and test sets
auc_roc_train = roc_auc_score(y_train_resampled, y_prob_train)
auc_roc_test = roc_auc_score(y_test, y_prob_test)

# Print evaluation results
print(f"Decision Tree Model Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Train AUC-ROC: {auc_roc_train:.4f}")
print(f"Test AUC-ROC: {auc_roc_test:.4f}")

# Plot ROC curve for both Train and Test sets
fpr_train, tpr_train, _ = roc_curve(y_train_resampled, y_prob_train)
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test)

plt.figure(figsize=(8,6))
plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'Train AUC = {auc_roc_train:.4f}')
plt.plot(fpr_test, tpr_test, color='red', lw=2, label=f'Test AUC = {auc_roc_test:.4f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Decision Tree After SMOTE')
plt.legend(loc="lower right")

# Save the plot to the local directory
output_path = "decision_tree_roc_curve_after_smote.png"
plt.savefig(output_path, dpi=300)  # Save with high resolution
plt.show()

print(f"ROC curve saved to: {output_path}")